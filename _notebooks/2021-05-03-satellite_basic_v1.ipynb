{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 01_satellite_basic.ipynb","provenance":[{"file_id":"https://github.com/micmacIGN/Documentation/blob/master/Tutorials/TutoJupyter/01_satellite_basic.ipynb","timestamp":1620055406868}],"collapsed_sections":["rYhBGH5kgCjC"],"toc_visible":true,"authorship_tag":"ABX9TyNIJeFORGqwF+weAihHIXR6"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gTEJ6-R8AY53"},"source":["# The satellite-image-based surface reconstruction in `MicMac`\n","\n","In this tutorial we will introduce you to satellite image processing in MicMac. The goal of the exercise is to compute the surface and generate an orthohoto given a set of *modern* satellite images and their RPC geolocalisations. After setting-up MicMac and downloading the dataset, the pipeline is as follows:\n","  1. Tie-points extraction\n","  2. RPC-bundle adjustement\n","  3. Surface computation\n","    \n","    3.1. ***Method1***: Matching in object geometry\n","\n","    3.3. ***Method2***: Matching in image geometry and fusion\n","\n","\n","When you're done, find the next tutorial here: \n","[02_satellite_epipolaire.ipynb](https://github.com/micmacIGN/Documentation/blob/master/TutoJupyter/02_satellite_epipolaire.ipynb)\n","\n","<br>\n","\n","*Contact: ewelina.rupnik(at)ign.fr*"]},{"cell_type":"markdown","metadata":{"id":"0MbhvL1TEj-8"},"source":["## Projet set-up"]},{"cell_type":"markdown","metadata":{"id":"SaENxS9LAtG9"},"source":["### Download and compile MicMac & dependencies"]},{"cell_type":"code","metadata":{"id":"TAEcGenixfhW"},"source":["import os\n","from os.path import exists, join, basename, splitext\n","import numpy as np \n","import cv2\n","import matplotlib.pyplot as plt   \n","\n","Dependencies_install = True\n","MicMac_clone = True\n","MicMac_cmake = True \n","MicMac_build = True\n","\n","YOUR_PATH = '/content/'#MyDrive/micmac/satellites/'\n","!cd $YOUR_PATH\n","!pwd\n","\n","\n","if Dependencies_install:\n","  !apt update\n","  !apt install -y cmake\n","  !pip install dlib\n","  !apt-get install imagemagick proj-bin exiv2\n","  !pip install wget gdown\n","\n","if MicMac_clone:\n","  if not exists(YOUR_PATH+'micmac/'):\n","    git_repo_url = 'https://github.com/micmacIGN/micmac.git'\n","    !git clone $git_repo_url\n","\n","if MicMac_cmake:\n","  !cd micmac\n","  if not exists(YOUR_PATH+'micmac/build'):\n","    !mkdir $YOUR_PATH\"micmac/build\"\n","\n","  !cd $YOUR_PATH\"micmac/build\"\n","  !cmake $YOUR_PATH\"micmac\" -DBUILD_POISSON=OFF\n","\n","  if MicMac_build:\n","    !make install -j28\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVl6Wvjv1OL3"},"source":["### Add environmental variable & download the dataset\n","\n","The dataset consists of: \n","  * 4 images (tif)\n","  * 4 corresponding RPCs (xml)\n","  * `WGS84toUTM.xml` with the definition of a projection coordinate system (*proj4* format)"]},{"cell_type":"code","metadata":{"id":"clCi0abZy8Tj"},"source":["import os\n","os.environ['PATH'] += \":/content/micmac/bin/\"\n","!echo $PATH\n","\n","# if you can see the commands printed to the screen, everything is OK\n","!mm3d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjvEDaix4WZ4"},"source":["# download data\n","dataset_url = 'https://drive.google.com/uc?id=18hmQL5kIqhcnR5ahp8IUsMZxLv7jvjgB' \n","!gdown $dataset_url -O \"satellite_data.tar.gz\" \n","\n","# unpack\n","if not exists(YOUR_PATH+'satellite_data'):\n","  !mkdir $YOUR_PATH'satellite_data'\n","!tar -xf satellite_data.tar.gz -C $YOUR_PATH'satellite_data'\n","%cd $YOUR_PATH'satellite_data'\n","\n","# utility functions to visualise tie-points\n","utils_url='https://drive.google.com/uc?id=1ATO1Nz_aXApxVnm6l7x1xappGXtcjuvp'\n","!gdown $utils_url -O \"mm3d_utils.py\"   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAhJxwLpFIJP"},"source":["## 1. Extract SIFT tie-points\n","\n","  - **computation strategy**: there exist several predefined *strategies* to compute tie-points: `Line`, `All`, `MulScale`, `File`. We will use the `All` strategy where tie-points are searched between all possible pairs. Refer to MicMac documentation for the other modes. \n","  - **image resolution**: tie-points extraction is very costly, and to limit the computation time we usually downsampled the images; in this example, indicate resolution of `-1` which means full-resolution images; otherwise, if set to, e.g., `2000`, the images will be downsampled such that the larger image dimension (typically the width) will have `2000` pixels; the other dimension will have a size that is proportionally smaller;\n","  - **ExpTxt=1**: the extracted tie-points will be saved in a text format (as opossed to the default dat format)\n","  - **results**: tie-points are stored in the `Homol` directory. For instance, tie-points correponding to image `Im1.tif` will be stored in `Homol/PastisIm1.tif/` directory. If `Im1.tif` overlaps with `Im2.tif` and `Im3.tif`, their tie-points will be stored in `Homol/PastisIm1.tif/Im2.tif.dat` and `Homol/PastisIm1.tif/Im3.tif.dat`, respectively. If you chose to export in the text format, the `dat` extension will be replaced with `txt`.\n","\n","\n","*Note: Intermediary results are stored in the `Pastis` directory. It takesa  significant amount of space and is not used at later processing stages, therefore you may delete it.*"]},{"cell_type":"code","metadata":{"id":"IiL8_kB9DgPX"},"source":["!mm3d Tapioca All .*tif -1 ExpTxt=1 @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_YAQhPoFe8E"},"source":["### Visualise tie-points\n","\n","Read any pair of images and visalise their tie-points."]},{"cell_type":"code","metadata":{"id":"7_EGFEO1FaJj"},"source":["import mm3d_utils \n","\n","aIm1 = cv2.imread('TPMM_0435.tif',cv2.IMREAD_IGNORE_ORIENTATION)\n","aIm2 = cv2.imread('TPMM_0566.tif',cv2.IMREAD_IGNORE_ORIENTATION) \n"," \n","TPtsVec = mm3d_utils.ImportHom(\"Homol/PastisTPMM_0435.tif/TPMM_0566.tif.txt\") \n"," \n","mm3d_utils.plot_images([np.asarray(aIm1),np.asarray(aIm2)]) \n","mm3d_utils.plot_tiepts2(np.asarray(TPtsVec,dtype=float))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZfUdTkjICL4"},"source":["## 2. RPC-bundle adjustment"]},{"cell_type":"markdown","metadata":{"id":"t_zNRmlYURo1"},"source":["### Read the RPCs in DIMAP format\n","\n","This function reads the DIMAP format RPCs and converts it to a *MicMac* format. Several parameters are specified here:\n","\n","  - `(.*).tif` this is the pattern of input images (note the dot preceding the star which is the posix convention)\n","\n","  - `\\$1.xml` is the corresponding pattern of RPC files; I use here a regular expression that associates the image name with its corresponding RPC file name; you may also run the command independently for each image if you're not familiar with regular expressions;\n","\n","  - `RPC-d0` is the directory name where the converted files will be stored; it will serve as input in the following step, i.e., the bundle adjustment;\n","\n","  - `Degre=0`, the degree of the polynomial correction;\n","\n","  By choosing a zero-degree polynomial we will correct the satellite's geolocalisation by modelling a 3D image shift; please refer to [Rupnik et al., 2016] for more on the method.\n","\n","  - `ChSys=WGS84toUTM.xml` definition of the projection coordinate sytem; MicMac expects that the processing coordinate frame is euclidean and all three coordinates have the same unit. The RPCs are expressed in geographical coordinates which are neither euclidean, nor unique in terms of units. To overcome that, MicMac will transfer, on the fly, the RPCs to a user-defined coordinate system, in this exemple defined in the `WGS84toUTM.xml` file. The definition of the coordinate system follows the `proj4` library convention. You can retrieve the code corresponding to the coordinate frame of your interest from `https://spatialreference.org/`\n","\n","\n","\n","<sub> Rupnik, E., Deseilligny, M.P., Delorme, A. and Klinger, Y., 2016. Refined satellite image orientation in the free open-source photogrammetric tools Apero/Micmac. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 3, p.83.<sub>"]},{"cell_type":"code","metadata":{"id":"ZMTM1p2dIE78"},"source":["!mm3d Convert2GenBundle -help @ExitOnBrkp\n","!mm3d Convert2GenBundle \"(.*).tif\" \"\\$1.xml\" RPC-d0 ChSys=WGS84toUTM.xml Degre=0  @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QcgmeZ2SIGxD"},"source":["### Run the adjustment\n","\n","Within the bundle adjustemnt, MicMac will estimate the parameters of $D_x$ and $D_y$ functions, that are the polynomials you have defined in `Convert2GenBundle` method. In this example our observations are the tie-points. It is also possible to include ground control points.\n","\n","\n","$\\begin{equation}\n","\\begin{split}\n","y = g (\\phi, \\lambda, h) +  {D_y (x,y)}\\\\\n","x = h (\\phi, \\lambda, h) +  {D_x (x,y)}\n","\\end{split}\n","\\end{equation}$\n","\n","where $g$ and $h$ are the RPC functions, and \n","\n","$\\begin{equation}\n","\\begin{split}\n","D_y (x,y) = \\sum_{i=0}^m \\sum_{j=0}^n a_{ij} \\cdot x^i y^j \\\\\n","D_x (x,y) = \\sum_{i=0}^m \\sum_{j=0}^n b_{ij} \\cdot x^i y^j\n","\\end{split}\n","\\end{equation}$\n","\n","\n","The input parameters:\n","- `RPC-d0` is the folder with the initial geolocalisation\n","- `RPC-d0_adj` is the folder where the adjusted geoloc is saved\n","- `ExpTxt=1` indicates that tie-points are stored in text format "]},{"cell_type":"code","metadata":{"id":"LYWMUoyqI4VH"},"source":["!mm3d Campari \".*tif\" RPC-d0 RPC-d0-adj ExpTxt=1 @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bqsrw2t_CDgX"},"source":["### Interpreting the results\n","\n","One way to asses the quality of the adjustment is to look at the tie-points residual (for more sophisticated quality estimates see `MMTestOrient` in MicMac documentation). \n","\n","The bundle adjustment is carried out in several iterations. Let's look at image `TPMM_0435.tif` in the last iteration:\n","\n","> `RES:[TPMM_0435.tif][g] ER2 0.24636 Nn 100 Of 11753 Mul 5171 Mul-NN 5171 Time 1.15821`\n","\n","* `0.24636` pixels is the mean residual calculated over all tie-points (i.e., $\\sigma$ of the bundle)  \n","\n","* `Nn 100` means that 100$\\%$ of tie-points were considered as inliers\n","\n","* `11753` there were as many tie-points found\n","\n","* `5171` there were as many multiple tie-points found (out of the `11753`), i.e., tie-points observed in at least 3 images;"]},{"cell_type":"markdown","metadata":{"id":"FsoAkSMnJdmT"},"source":["## 3. Surface reconstruction\n","\n","We will now calculate the surface with the semi-global dense image matching [Deseilligny \\& Paparoditis, 2006]. \n","\n","\n","\n"," <sub> Deseilligny, M. and Paparoditis, N., 2006. A multiresolution and optimization-based image matching approach: An application to surface reconstruction from SPOT5-HRS stereo imagery. Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 36(1/W41), pp.1-5."]},{"cell_type":"markdown","metadata":{"id":"IXP2guTC4GYj"},"source":["### ***Method1***: Matching in object geometry\n","\n","The computation will be carried out in the so-called terrain geometry, where the optimization is defined in the `(X,Y)` of the object space, and we are looking for the most optimal `Z`-coordinates (see Figure below). This geometry is well adapted to 2.5D surface computation.\n","\n","<center>\n","  <img src=\"https://drive.google.com/uc?id=17tagHuLrWpYJYBgkut2obB6M2xxSM-C8\" height=220pix/>\n","  <br> \n","</center>\n"," \n","\n","\n","<center>\n","Figure. Matching in object geometry. \n","  <br> \n","</center>\n","\n"," The input parameters are:\n","  - `UrbanMNE` is a predefined term and it defines a number of processing parameters (e.g., low regularization, small matching wodows, terrain geometry)\n","\n","  - `.*tif` is the image set that will be used in the processing\n","\n","  - `RPC-d0-adj` is the name of the directory containing the geoloclisation\n","\n","  - `SzW=2` defines the matching window size, i.e., with size set to 2, the window size is ```5x5```\n","\n","  - `Regul` is the regularization term $\\alpha$; in `UrbanMNE` it is by default set to `0.02` becase in urban zones we're typically interested in reconstructing fine details; the dataset used in this example, however, represents a smooth surface so we're just fine with a more agressive regularization; moreover, our images are quite noisy and by adding more regularization we will avoid noisy surface reconstructions;\n","\n","  - `DoOrtho=1`, this parameters wil force MicMac to create individual orthomosaic, i.e., rectify each image; the rectified images are stored in `Ortho-MEC-Malt/Ort_*.tif`; to create the final orthophotomap we will still need to do mosaicing with `Tawny` (later in this tutorial);\n","\n","  - `NbVI=2` sets the necessary minimum number of images for MicMac to compute the surface; by default the value is set to 3 which means that in areas with only two overlapping images, the surface will not be computed; \n","\n","  - `EZA=1`, this parameter will force the output surface raster to save the Z-coordinates in their absolute values; without explicitly forcing MicMac to do that, to avoid having to store large values inside the raster tiff, it will apply a normalisation (normalisation parameters `OrigineAlti` and `ResolutionAlti` are to be found in the accompanying xml files);"]},{"cell_type":"code","metadata":{"id":"_cCUL3EhJaKT"},"source":["!mm3d Malt UrbanMNE \".*tif\" RPC-d0-adj SzW=2 Regul=0.2 DoOrtho=1 NbVI=2 EZA=1 @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYhBGH5kgCjC"},"source":["#### Reading the output files\n","\n","The matching is carried out at multi-resolutions, i.e., we first calculate the surface using low resolution images (top-most level of the image pyramid), then we propagate the solution to lower levels and refine it, so long we have not reached the bottom of the image pyramid. The surface reconstructions at each level are stored inside the `MEC-Malt` directory. Here's how to decipher the files:\n","\n","- `MEC-Malt/Z_Num8_DeZoom1_STD-MALT.tif` represents the surface raster at the highest resolution; \n","- `MEC-Malt/Z_Num8_DeZoom1_STD-MALT.xml` is its metadata file that encodes the georeferencing; let's assume you'd like to convert a pixel $(i,j)$ from its image coordinates to its georeferenced coordinates (i.e., object coordinates):\n","\n","\n","> `Z-coordinate normalised` : $Z^{img}_{i,j} = Z\\_Num\\_File^{img}(i,j)$, \n","\n","> `Z-coordinate` : $Z^{obj}_{i,j} = OrigineAlti + ResolutionAlti \\cdot Z^{img}_{i,j}$\n","\n","> `XY-coordinates` : $(X,Y) = OriginePlani + ResolutionAlti \\cdot (i,j)$\n","\n","\n","- `MEC-Malt/Masq_STD-MALT_DeZoomX.tif` is a binary mask file that is a result of your input mask (if you used one) and a mask that is automatically calculated in the matching optimisation phase;\n","\n","- `MEC-Malt/Correl_STD-MALT_Num_X.tif` are the images with storing the correlation scores (it is not pure correlation, it is the correlation store after the aggregation step)\n"]},{"cell_type":"markdown","metadata":{"id":"qHNEhIUvMn5A"},"source":["#### Create a grayshaded DSM\n","\n","Represent the surface in form of a grayshading. To visually asses the quality of your surface, it is much more intuitive than just looking at the depth/Z image. "]},{"cell_type":"code","metadata":{"id":"Ur2W9k7HODYM"},"source":["!mm3d GrShade MEC-Malt/Z_Num8_DeZoom1_STD-MALT.tif ModeOmbre=IgnE Mask=MEC-Malt/Masq_STD-MALT_DeZoom1.tif @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnCvfVssT42o"},"source":["#### Visualise the grayshaded surface"]},{"cell_type":"code","metadata":{"id":"Bf4qswIpR66R"},"source":["surface_shade_im = cv2.imread(\"MEC-Malt/Z_Num8_DeZoom1_STD-MALTShade.tif\",cv2.IMREAD_IGNORE_ORIENTATION)\n","\n","fig, ax = plt.subplots(figsize=(30, 10))\n","ax.imshow(surface_shade_im,cmap=\"gray\") \n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXWniQ1VSkTs"},"source":["#### Generate an orthophotmap\n","\n","`Tawny` will mosaic the per-image orthopĥotomosaics created in `Malt` (i.e., `Ortho-MEC-Malt/Ort_*.tif`), during matching. It will additionally perform some basic radiometry equalization. The output orthoimage is stored in `Ortho-MEC-Malt/Orthophotomosaic.tif`, and its georeferencing is stored in `Ortho-MEC-Malt/Orthophotomosaic.twf`."]},{"cell_type":"code","metadata":{"id":"gXgFkQpSSj26"},"source":["!mm3d Tawny Ortho-MEC-Malt/ @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkWUZMKZTscD"},"source":["#### Visualise the orthophoto"]},{"cell_type":"code","metadata":{"id":"cOFu6Y20SsGU"},"source":["ortho_im = cv2.imread(\"Ortho-MEC-Malt/Orthophotomosaic.tif\",cv2.IMREAD_IGNORE_ORIENTATION)\n","\n","fig, ax = plt.subplots(figsize=(30, 10))\n","ax.imshow(ortho_im,cmap='gray') \n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OP2wNAdM4zVG"},"source":["### ***Method2***: Multiview matching in image geometry and fusion\n","\n","The computation will be carried out in the image geometry, where the optimization is defined in the `(x,y)` of the image space, and we are looking for the most optimal `depths` (see Figure below). This geometry is well adapted to true 3D surface reconstruction. Because the individual reconstructions are computed in image coordinate frames, a fusion will but carried out at the end.\n","\n","<center>    \n","  <img src=\"https://drive.google.com/uc?id=1bgYWeaQlpMI7ghQLubogHph7hkRR4lE-\" height=220pix/>\n","  <br> \n","</center>\n"," \n","\n","<center>\n","Figure. Matching in image geometry. \n","  <br> \n","</center>\n","\n","The multiview pipeline is as follows [Rupnik et al., 2018]:\n","\n","1. Extract tie-points and do RPC-bundle adjustement (done previously)\n","2. Do N per-stereo (or per-M image as we are not bound by the number of images) dense matching (`Malt GeomImage`)\n","3. Transform the N depth maps to a common coordinate frame (`NuageBascule`)\n","4. Fuse the N depth maps into one (`SMDM`)\n","\n","\n","\n","<center>    \n","  <img src=\"https://drive.google.com/uc?id=1nqgkofwm-ksDDadzqDUHwSMYHoie_s8V\" height=220pix/>\n","  <br> \n","</center>\n"," \n","\n","<center>\n","Figure. Multiview image matching and fusion pipeline. \n","  <br> \n","</center>\n","\n","<sub> Rupnik, E., Pierrot-Deseilligny, M. and Delorme, A., 2018. 3D reconstruction from multi-view VHR-satellite images in MicMac. ISPRS Journal of Photogrammetry and Remote Sensing, 139, pp.201-211.\n"]},{"cell_type":"markdown","metadata":{"id":"kXMNqwfG8uGt"},"source":["#### Do two per-triplet image matching\n","\n","We will compute two surfaces using two different subsets of the images. \n","\n","Input parameters:\n","\n","  - `GeomImage` is a predefined term and it defines a number of processing parameters (e.g., low regularization, small matching wodows, image geometry)\n","\n","  - `TPMM_(0435|0566|1088).*tif` is the image set that will be used in the processing; in this example it is an image triplet; we take 3 **consecutive images** to make sure that the $\\frac{B}{H}$ ratios in the set are relatively small\n","\n","  - `RPC-d0-adj` is the name of the directory containing the geolocalisation\n","\n","  - `Master=TPMM_0566.tif` is the master image, i.e., the optimization is defined over each pixel of that image\n","\n","  - `SzW=2` and `Regul=0.2`, similarily to `Method1`, we add regularization and use bigger correlation windows because: (1) the geometry of the surface is smooth (i.e., no discontinuities), and (2) the images are quiet noisy;\n","\n","  - `NbVI=2` sets the necessary minimum number of images for MicMac to compute the surface; by default the value is set to 3 which means that in areas with only two overlapping images, the surface will not be computed; "]},{"cell_type":"code","metadata":{"id":"0BW4AO4r8tNZ"},"source":["#the first triplet\n","!mm3d Malt GeomImage \"TPMM_(0435|0566|1088).*tif\" RPC-d0-adj Master=TPMM_0566.tif SzW=1 Regul=0.1 NbVI=2 ZPas=1 @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ivWg2EuR890m"},"source":["# the second triplet\n","!mm3d Malt GeomImage \"TPMM_(0566|1088|1216).*tif\" RPC-d0-adj Master=TPMM_1088.tif SzW=1 Regul=0.1 NbVI=2 ZPas=1   @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5R2p16E9iX0"},"source":["#### Transform the depth maps to a common coordinate frame\n","\n","  * The $1^{st}$ command `Malt UrbanMNE` does image matching in ground geometry (we've used it before), but since we indicate `DoMEC=0` it will not calculate the matching, all it will do is to create metadata defining the coordinate frame of the ground geometry. The output, as before, is stored in `MEC-Malt` folder;\n","\n","  * The $2^{nd}$ command `NuageBascule` will apply the transformation; the parameters are:\n","    * `MM-Malt-Img-TPMM_0566/NuageImProf_STD-MALT_Etape_8.xml` is the metadata file defining the input coordinate frame\n","    * `MEC-Malt/NuageImProf_STD-MALT_Etape_8.xml` is the metadata file defining the target coordinate frame (i.e., it is the terrain geometry)\n","    * `Fusion/DSM_Tri1.xml` is the output metadata file, i.e., the input file transformed to the target coordinate frame; it will be accompagned by several other files containing the surface itself, the mask and the correlation image (see the inside of the `Fusion` folder);"]},{"cell_type":"code","metadata":{"id":"1VoGkDw59uxH"},"source":["# define the common frame\n","!mm3d Malt UrbanMNE \".*tif\" RPC-d0-adj DoMEC=0  @ExitOnBrkp\n","\n","# create a directory that will store the fused surface\n","!mkdir Fusion\n","\n","# do 3D spatial similarity of the first triplet depth map\n","!mm3d NuageBascule MM-Malt-Img-TPMM_0566/NuageImProf_STD-MALT_Etape_8.xml MEC-Malt/NuageImProf_STD-MALT_Etape_8.xml Fusion/DSM_Tri1.xml @ExitOnBrkp\n","\n","# do 3D spatial similarity of the second triplet depth map\n","!mm3d NuageBascule MM-Malt-Img-TPMM_1088/NuageImProf_STD-MALT_Etape_8.xml MEC-Malt/NuageImProf_STD-MALT_Etape_8.xml Fusion/DSM_Tri2.xml @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACxpj5lp90To"},"source":["#### Fuse the individual depth maps\n","\n","The fusion takes all the surfaces specified by the regular expression and merges it. The fusion takes into account the correlation images and treats it as confidence maps.\n","\n","Input parameters:\n","\n","* `Fusion/DSM_Tri.*xml` the subset of surfaces that will be merged; \n","\n","Tha result is saved to `Fusion/Fusion_Prof.tif`, there is a corresponding mask and a correlation map named with `_Mask` and `Correl` postfixes, respectively."]},{"cell_type":"code","metadata":{"id":"y_TNDCiU-fCA"},"source":["!mm3d SMDM Fusion/DSM_Tri.*xml  @ExitOnBrkp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7wDXFMY1-oVR"},"source":["#### Visualise in grayshade and export to ply"]},{"cell_type":"code","metadata":{"id":"EpXQgdln-rs7"},"source":["# grayshade\n","!mm3d GrShade Fusion/Fusion_Prof.tif Out=Fusion/Fusion_GShade.tif ModeOmbre=IgnE @ExitOnBrkp\n","\n","surface_fused_shade_im = cv2.imread(\"Fusion/Fusion_GShade.tif\",cv2.IMREAD_IGNORE_ORIENTATION)\n","\n","fig, ax = plt.subplots(1,2,figsize=(15, 15))\n","ax[0].imshow(surface_fused_shade_im,cmap=\"gray\") \n","ax[1].imshow(surface_shade_im,cmap=\"gray\") \n","plt.tight_layout()\n","\n","# export to ply\n","#!mm3d Nuage2Ply Fusion/Fusion.xml Out=Fusion.ply"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qIxF3u-J3S75"},"source":["!mm3d GrShade MM-Malt-Img-TPMM_1088/Z_Num8_DeZoom1_STD-MALT.tif Mask=MM-Malt-Img-TPMM_1088/Masq_STD-MALT_DeZoom1.tif  ModeOmbre=IgnE @ExitOnBrkp\n","\n","surface_test_shade_im = cv2.imread(\"MM-Malt-Img-TPMM_1088/Z_Num8_DeZoom1_STD-MALTShade.tif\",cv2.IMREAD_IGNORE_ORIENTATION)\n","\n","fig, ax = plt.subplots(figsize=(15, 15))\n","ax.imshow(surface_test_shade_im,cmap=\"gray\")  \n","plt.tight_layout()\n"],"execution_count":null,"outputs":[]}]}